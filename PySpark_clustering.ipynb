{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task description**: find patterns in large amount of short texts (sms/push and etc.)\n",
    "* **Data**: 5Gb csv file with history\n",
    "* **Idea**: Create groups of elements with similar text\n",
    "* **Comment**: work was performed on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:28:29.337738Z",
     "start_time": "2019-07-25T14:28:26.414736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.sql import SparkSession, Row
    "from pyspark.sql.functions import isnull, isnan, ltrim, rtrim, lower, split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:28:33.830644Z",
     "start_time": "2019-07-25T14:28:29.340168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create PySpark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:29:32.610121Z",
     "start_time": "2019-07-25T14:28:33.851534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data into dataframe. inferSchema â€“ infers the input schema automatically from data.\n",
    "df_init = spark.read.csv('Input_file.csv', sep = ';', header = True, inferSchema = True)\n",
    "df_init.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:29:44.205998Z",
     "start_time": "2019-07-25T14:29:32.612445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count how many rows in our dataframe\n",
    "df_init.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:29:44.220474Z",
     "start_time": "2019-07-25T14:29:44.207790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look what columns do we have\n",
    "df_init.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target colums - \"text\" and \"system_id\" (for information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:01.816237Z",
     "start_time": "2019-07-25T14:29:44.222211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if \"text\" has missing values\n",
    "df_init.filter(isnull('text') | isnan('text')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:19.997873Z",
     "start_time": "2019-07-25T14:30:01.818067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy nessasary information to new dataframe. Delete missing values. Check again if \"text\" has missing values.\n",
    "df = df_init[['external_id','text']]\n",
    "df = df.dropna()\n",
    "df.filter(isnull('text') | isnan('text')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:20.108559Z",
     "start_time": "2019-07-25T14:30:20.000069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add new column\n",
    "df = df.withColumn('text_clean', df.text)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:20.120787Z",
     "start_time": "2019-07-25T14:30:20.111558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function for normalizing whitespaces (delete multiple + left and right trim)\n",
    "def normalizeSpace(df,colName):\n",
    "\n",
    "    # Left and right trim\n",
    "    df = df.withColumn(colName,ltrim(df[colName]))\n",
    "    df = df.withColumn(colName,rtrim(df[colName]))\n",
    "\n",
    "    # This is faster than regexp_replace function!\n",
    "    def normalize(row,colName):\n",
    "        data = row.asDict()\n",
    "        text = data[colName]\n",
    "        spaceCount = 0;\n",
    "        Words = []\n",
    "        word = ''\n",
    "\n",
    "        for char in text:\n",
    "            if char != ' ':\n",
    "                word += char\n",
    "            elif word == '' and char == ' ':\n",
    "                continue\n",
    "            else:\n",
    "                Words.append(word)\n",
    "                word = ''\n",
    "\n",
    "        if len(Words) > 0:\n",
    "            data[colName] = ' '.join(Words)\n",
    "\n",
    "        return Row(**data)\n",
    "\n",
    "    df = df.rdd.map(lambda row: normalize(row,colName)).toDF()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:20.128358Z",
     "start_time": "2019-07-25T14:30:20.124094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function for clean text for better clustering\n",
    "def clean_text(df, column):\n",
    "    # Remove numbers\n",
    "    df = df.withColumn(column, regexp_replace(column, r'[0-9]', ' '))\n",
    "    # Remove punctuation\n",
    "    df = df.withColumn(column, regexp_replace(column, '\\p{Punct}', ' '))\n",
    "    # Set lower case\n",
    "    df = df.withColumn(column, lower(col(column)))\n",
    "    # Normalize whitespaces\n",
    "    df = normalizeSpace(df,column)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:20.133321Z",
     "start_time": "2019-07-25T14:30:20.130807Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing it may be useful to use not all data\n",
    "#df = df.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:21.178247Z",
     "start_time": "2019-07-25T14:30:20.135296Z"
    }
   },
   "outputs": [],
   "source": [
    "df = clean_text(df,'text_clean')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:21.323816Z",
     "start_time": "2019-07-25T14:30:21.180061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create new column with \"array text\" for PySpark K-means\n",
    "df = df.withColumn('text_array',split('text_clean',' '))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:30:21.386813Z",
     "start_time": "2019-07-25T14:30:21.325667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "n_clust = 50\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"text_array\", outputCol=\"features\", vocabSize = 10000)\n",
    "\n",
    "# K-Means uses column \"features\" by default\n",
    "model = KMeans(k = n_clust, maxIter = 10000, seed = 1)\n",
    "\n",
    "pipeline = Pipeline(stages=[cv, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:41:25.967542Z",
     "start_time": "2019-07-25T14:30:21.389167Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)\n",
    "results = model.transform(df)\n",
    "results.select('text','prediction').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:41:26.180983Z",
     "start_time": "2019-07-25T14:41:26.079068Z"
    }
   },
   "outputs": [],
   "source": [
    "df_out = results.select('text','prediction')\n",
    "df_out.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T13:14:06.840422Z",
     "start_time": "2019-07-24T13:09:35.004108Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Simply export dataframe. It will be folder with many csvs.\n",
    "df_out.write.csv(\"Clusters.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Create one csv for each cluster\n",
    "for i in range(n_clust):\n",
    "    csv_name = \"text\" + str(i) + \".csv\"\n",
    "    df_out.coalesce(1).filter(df_out.prediction == i).select('text').write.csv(csv_name, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T14:10:39.787085Z",
     "start_time": "2019-07-25T13:57:24.781920Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3) Pivot dataframe. Put each class in separate column.\n",
    "df_pivot = df_out.filter(df_out.prediction == 0).select('text').selectExpr(\"text as t0\")\n",
    "df_pivot = df_pivot.withColumn(\"id0\", monotonically_increasing_id())\n",
    "\n",
    "for i in range(1,n_clust):\n",
    "    rename_column = \"text as t\" + str(i)\n",
    "    \n",
    "    df_temp = df_out.filter(df_out.prediction == i).select('text').selectExpr(rename_column)\n",
    "    df_temp = df_temp.withColumn(\"id_\", monotonically_increasing_id())\n",
    "    \n",
    "    df_pivot = df_pivot.join(df_temp, df_pivot.id0 == df_temp.id_, 'outer')\n",
    "    df_pivot = df_pivot.drop('id_')\n",
    "\n",
    "df_pivot = df_pivot.drop('id0')\n",
    "df_pivot.show(5)\n",
    "\n",
    "#df_pivot.write.csv(\"Clusters.csv\", header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
